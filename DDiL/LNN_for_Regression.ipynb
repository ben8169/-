{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9qxYHVEbDpg2RMZf/JVtM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben8169/Study/blob/main/DDiL/LNN_for_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #for d2l\n",
        "# !pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
        "# !pip install d2l==1.0.0b0"
      ],
      "metadata": {
        "id": "VBf2I7pBd9CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>3.LNN_for_Regression</h2>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WrCVXY7CUniq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>3.1Linear Regression</h3>\n",
        "<li> Gradient Descent Point ->  iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function.\n",
        "<li>Mini batch size -> 2^n(32~256)\n",
        "<li>MSGD - > take minibatch, get gradient, multiply lr, update\n",
        "<li><b>Generalization</b> >> minimizing the loss at deep learning\n"
      ],
      "metadata": {
        "id": "SKTqAztacjzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Normal Distribution and Squared Loss</h3>\n",
        "<li> Gradient Descent Point ->  iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function.\n",
        "<li>Mini batch size -> 2^n(32~256)\n",
        "<li>MSGD - > take minibatch, get gradient, multiply lr, update\n",
        "<li><b>Generalization</b> >> minimizing the loss at deep learning"
      ],
      "metadata": {
        "id": "1m_B3Gijcsx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>3.2 Object-Oriented Design for Implementation</h3>\n",
        "<li>python practice"
      ],
      "metadata": {
        "id": "z0BigjQjcsaC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9KBhwEaOSwMG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Module(nn.Module, d2l.HyperParameters):\n",
        "    \"\"\"The base class of models.\"\"\"\n",
        "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.board = ProgressBoard()\n",
        "\n",
        "    def loss(self, y_hat, y):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, X):\n",
        "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
        "        return self.net(X)\n",
        "\n",
        "    def plot(self, key, value, train):\n",
        "        \"\"\"Plot a point in animation.\"\"\"\n",
        "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
        "        self.board.xlabel = 'epoch'\n",
        "        if train:\n",
        "            x = self.trainer.train_batch_idx / \\\n",
        "                self.trainer.num_train_batches\n",
        "            n = self.trainer.num_train_batches / \\\n",
        "                self.plot_train_per_epoch\n",
        "        else:\n",
        "            x = self.trainer.epoch + 1\n",
        "            n = self.trainer.num_val_batches / \\\n",
        "                self.plot_valid_per_epoch\n",
        "        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),\n",
        "                        ('train_' if train else 'val_') + key,\n",
        "                        every_n=int(n))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('loss', l, train=True)\n",
        "        return l\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('loss', l, train=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "Q1E4JW-Eexkm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataModule(d2l.HyperParameters):\n",
        "    \"\"\"The base class of data.\"\"\"\n",
        "    def __init__(self, root='../data', num_workers=4):\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.get_dataloader(train=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.get_dataloader(train=False)"
      ],
      "metadata": {
        "id": "yuZdU2V1ezoW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(d2l.HyperParameters):\n",
        "    \"\"\"The base class for training models with data.\"\"\"\n",
        "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
        "        self.save_hyperparameters()\n",
        "        assert num_gpus == 0, 'No GPU support yet'\n",
        "\n",
        "    def prepare_data(self, data):\n",
        "        self.train_dataloader = data.train_dataloader()\n",
        "        self.val_dataloader = data.val_dataloader()\n",
        "        self.num_train_batches = len(self.train_dataloader)\n",
        "        self.num_val_batches = (len(self.val_dataloader)\n",
        "                                if self.val_dataloader is not None else 0)\n",
        "\n",
        "    def prepare_model(self, model):\n",
        "        model.trainer = self\n",
        "        model.board.xlim = [0, self.max_epochs]\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, model, data):\n",
        "        self.prepare_data(data)\n",
        "        self.prepare_model(model)\n",
        "        self.optim = model.configure_optimizers()\n",
        "        self.epoch = 0\n",
        "        self.train_batch_idx = 0\n",
        "        self.val_batch_idx = 0\n",
        "        for self.epoch in range(self.max_epochs):\n",
        "            self.fit_epoch()\n",
        "\n",
        "    def fit_epoch(self):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "RJ7CiieSe1Ac"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>3.3. Synthetic Regression Data</h3>\n",
        "<li>\n",
        "\n"
      ],
      "metadata": {
        "id": "W8czen3xe-sg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZpeIHnq3e3Ws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}