{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset and Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1760k  100 1760k    0     0   110k      0  0:00:15  0:00:15 --:--:--  227k\n",
      "Number of non-spam emails in the dataset:\n",
      "    3672\n",
      "Number of spam emails in the dataset:\n",
      "    1500\n"
     ]
    }
   ],
   "source": [
    "! curl http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz -o enron1.tar.gz\n",
    "! tar -xf enron1.tar.gz enron1\n",
    "\n",
    "print(\"Number of non-spam emails in the dataset:\")\n",
    "! ls -1 enron1/ham/*.txt | wc -l\n",
    "print(\"Number of spam emails in the dataset:\")\n",
    "! ls -1 enron1/spam/*.txt | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/ben8169/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/ben8169/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/ben8169/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ben8169/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/ben8169/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "# Download the NLTK tokenizer models\n",
    "! pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ben8169/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to /Users/ben8169/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ben8169/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"names\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of emails = 5172\n",
      "# of labels = 5172\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "\n",
    "# init\n",
    "\"\"\"\n",
    "emails: a set of email\n",
    "labels: a set of label representing whetere the gien email is spam or ham\n",
    "  - spam: 1\n",
    "  - ham: 0\n",
    "\"\"\"\n",
    "\n",
    "emails, labels = [], []\n",
    "parition = 0\n",
    "\n",
    "# load spam dataset\n",
    "file_path = 'enron1/spam'\n",
    "\n",
    "for fname in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(fname, 'r', encoding='ISO-8859-1') as f:\n",
    "      emails.append(f.read())\n",
    "      labels.append(1)\n",
    "\n",
    "file_path = 'enron1/ham'\n",
    "for fname in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(fname, 'r', encoding='ISO-8859-1') as f:\n",
    "      emails.append(f.read())\n",
    "      labels.append(0)\n",
    "\n",
    "print('# of emails = {}\\n# of labels = {}'.format(len(emails), len(labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_only(word):\n",
    "    # Q1. Remove numbers and punctuations [0.5 points]\n",
    "    return word.isalpha()\n",
    "\n",
    "from nltk.corpus import names\n",
    "# Q2. Remove name entity [0.5 points]\n",
    "all_names = set(names.words())\n",
    "\n",
    "# lemmaization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# put all together to clean texts\n",
    "def clean_text(doc):\n",
    "    cleaned_doc = [lemmatizer.lemmatize(word.lower())\n",
    "                        for word in doc.split()\n",
    "                        if word.isalpha() and word not in all_names]\n",
    "    # Q3. For all words in doc, apply lowercase to words, remove number, punctuation, and name entity [2 points]\n",
    "\n",
    "    return ' '.join(cleaned_doc)\n",
    "\n",
    "cleaned_emails = [clean_text(doc) for doc in emails]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state=486)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=500)\n",
    "\n",
    "# Q4. Get counter vector for X_train and X_test [1 point]\n",
    "term_docs_train = cv.fit_transform(X_train)\n",
    "term_docs_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "adding a nonzero scalar to a sparse array is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m gnb \u001b[38;5;241m=\u001b[39m NaiveBayesClassifier()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Q5. Train and predict Naive Bayes model [1 point]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mgnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm_docs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m gnb_pred \u001b[38;5;241m=\u001b[39m gnb\u001b[38;5;241m.\u001b[39mpredict(term_docs_test)\n\u001b[1;32m     11\u001b[0m gnb_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(Y_test, gnb_pred)\n",
      "File \u001b[0;32m~/Downloads/2022313356_김지헌_midterm/naivebayes.py:51\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_index[lab] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_index[lab])\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prior()\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/2022313356_김지헌_midterm/naivebayes.py:92\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.get_likelihood\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         each_appear\u001b[38;5;241m.\u001b[39mappend(idx_row)\n\u001b[1;32m     91\u001b[0m     each_appear\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(each_appear)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood[label]\u001b[38;5;241m=\u001b[39m(\u001b[43meach_appear\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmoothing\u001b[49m)\u001b[38;5;241m/\u001b[39m(total_appear\u001b[38;5;241m+\u001b[39mword_num)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/scipy/sparse/_base.py:516\u001b[0m, in \u001b[0;36m_spbase.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# Now we would add this scalar to every element.\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madding a nonzero scalar to a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    517\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse array is not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m issparse(other):\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: adding a nonzero scalar to a sparse array is not supported"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from naivebayes import NaiveBayesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gnb = NaiveBayesClassifier()\n",
    "\n",
    "# Q5. Train and predict Naive Bayes model [1 point]\n",
    "\n",
    "gnb.fit(term_docs_train,Y_train)\n",
    "gnb_pred = gnb.predict(term_docs_test)\n",
    "gnb_accuracy = accuracy_score(Y_test, gnb_pred)\n",
    "print(\"Accuracy of the model is: {:.2f}\".format(gnb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m svc \u001b[38;5;241m=\u001b[39m SVMClassifier()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Q6. Train and predict SVM model [1 point]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43msvc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm_docs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m svc_pred \u001b[38;5;241m=\u001b[39m svc\u001b[38;5;241m.\u001b[39mpredict(term_docs_test)\n\u001b[1;32m     11\u001b[0m svc_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(Y_test, svc_pred)\n",
      "File \u001b[0;32m~/Downloads/2022313356_김지헌_midterm/svm.py:43\u001b[0m, in \u001b[0;36mSVMClassifier.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     40\u001b[0m y_i \u001b[38;5;241m=\u001b[39m y_[i]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Q1. Update the gradients [4 points - 1 point each] \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_i\u001b[38;5;241m*\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_param\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from svm import SVMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svc = SVMClassifier()\n",
    "\n",
    "# Q6. Train and predict SVM model [1 point]\n",
    "\n",
    "svc.fit(term_docs_train,Y_train)\n",
    "svc_pred = svc.predict(term_docs_test)\n",
    "svc_accuracy = accuracy_score(Y_test, svc_pred)\n",
    "print(\"Accuracy of the model is: {:.2f}\".format(svc_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Q7. Train and predict Logistic Regression model [1 point]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m lr\u001b[38;5;241m.\u001b[39minitialize_w(term_docs_train)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm_docs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m lr_pred \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mpredict(term_docs_test)\n\u001b[1;32m     12\u001b[0m lr_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(Y_test, lr_pred)\n",
      "File \u001b[0;32m~/Downloads/2022313356_김지헌_midterm/logisticregression.py:105\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m--> 105\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfwpass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(y \u001b[38;5;241m-\u001b[39m z)\n\u001b[1;32m    107\u001b[0m     w_grad, b_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbwpass(x, err)\n",
      "File \u001b[0;32m~/Downloads/2022313356_김지헌_midterm/logisticregression.py:52\u001b[0m, in \u001b[0;36mLogisticRegression.fwpass\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mGiven x, compute the equation below:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mz = w1*x1 + w2*x2 + ... wk*xk + b\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Q2. Calculate the z value using the equation above [1 point]\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     53\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(z)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from logisticregression import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Q7. Train and predict Logistic Regression model [1 point]\n",
    "\n",
    "lr.initialize_w(term_docs_train)\n",
    "lr.fit(term_docs_train,Y_train)\n",
    "lr_pred = lr.predict(term_docs_test)\n",
    "lr_accuracy = accuracy_score(Y_test, lr_pred)\n",
    "print(\"Accuracy of the model is: {:.2f}\".format(lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix:\n",
      "[[5 5]\n",
      " [7 3]]\n"
     ]
    }
   ],
   "source": [
    "from evaluation import my_evaluation_metrics\n",
    "\n",
    "y_true = np.array([1,0,0,1,0,1,1,1,1,0,0,0,0,0,1,1,1,0,0,1])\n",
    "y_pred = np.array([0,0,0,1,1,1,0,0,0,1,1,0,0,0,0,1,0,1,1,0])\n",
    "\n",
    "em = my_evaluation_metrics()\n",
    "\n",
    "print(\"The confusion matrix:\")\n",
    "print(em.my_confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.86304622, -1.15072829,  0.        ,  0.        ],\n",
       "       [-0.86304622, -0.86304622,  0.        ,  0.        ],\n",
       "       [-0.28768207, -0.28768207,  0.        ,  2.83825576]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import my_evaluation_metrics\n",
    "\n",
    "documents = ['car car car train train train train', 'car car car train train train cookie',\n",
    "             'car train coffee coffee coffee coffee coffee coffee coffee cookie cookie cookie cookie cookie cookie']\n",
    "\n",
    "em = my_evaluation_metrics()\n",
    "tf_idf = em.my_tf_idf(documents)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
