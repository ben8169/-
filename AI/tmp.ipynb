{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제목"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Machine Learning 기본 프로세스\n",
    "1. Hypothesis 설정 -> 데이터를 가장 잘 표현할 수 있는 함수 H(x) 설정\n",
    "2. Cost Function 설정 -> Hypothesis의 결과와 label간의 차이를 평가할 수 있는 함수 설정\n",
    "3. Learning Algorithm 설계 -> <b>Cost가 최소가 되도록 H(x)의 파라미터를 조정하는 것</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Perceptron\n",
    ">>##### Single Perceptron 구성\n",
    " - Activation Function = Step f, Sigmoid, ReLU 등, sum(wx+b)을 입력받아 정해진 출력을 내보내는 함수\n",
    "\n",
    "$S=X\\cdot W + b= \\begin{bmatrix}x_{1}&x_{2}&x_{3}\\end{bmatrix}\\begin{bmatrix}w_{1}\\\\w_{2}\\\\w_{3}\\end{bmatrix} + b = x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "W = np.array([4, 5, 6, 7, 8])\n",
    "B = 3\n",
    "\n",
    "print(X*W) # 각자의 원소끼리 곱해짐\n",
    "print(np.sum(W*X) + B) # 각자의 원소끼리 곱한 후 모두 더해짐\n",
    "print(np.matmul(W, X) + B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Linear Regression\n",
    "- Activation func \"f(x)=x\" 사용... H(x) = wx + b\n",
    "> Cost Function : Mean Squared Error (MSE)\n",
    "> $(1/m)*\\sum_{i=1}^{n}(h(x_{i})-y_{i})^{2}$\n",
    "- Cost func: label과 예측값 간의 차이(Error)를 수치화 하기 위한 함수.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Activation(x):\n",
    "    return W*x + B\n",
    "\n",
    "def Cost():\n",
    "    return np.mean((Activation(X) - Y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.5\n",
      "1198.5\n",
      "0 0 177.0\n",
      "0 1 154.0\n",
      "0 2 133.0\n",
      "0 3 114.0\n",
      "0 4 97.0\n",
      "0 5 82.0\n",
      "0 6 69.0\n",
      "0 7 58.0\n",
      "0 8 49.0\n",
      "0 9 42.0\n",
      "1 0 50.5\n",
      "1 1 38.5\n",
      "1 2 28.5\n",
      "1 3 20.5\n",
      "1 4 14.5\n",
      "1 5 10.5\n",
      "1 6 8.5\n",
      "1 7 8.5\n",
      "1 8 10.5\n",
      "1 9 14.5\n",
      "2 0 1.0\n",
      "2 1 0.0\n",
      "2 2 1.0\n",
      "2 3 4.0\n",
      "2 4 9.0\n",
      "2 5 16.0\n",
      "2 6 25.0\n",
      "2 7 36.0\n",
      "2 8 49.0\n",
      "2 9 64.0\n",
      "3 0 28.5\n",
      "3 1 38.5\n",
      "3 2 50.5\n",
      "3 3 64.5\n",
      "3 4 80.5\n",
      "3 5 98.5\n",
      "3 6 118.5\n",
      "3 7 140.5\n",
      "3 8 164.5\n",
      "3 9 190.5\n",
      "4 0 133.0\n",
      "4 1 154.0\n",
      "4 2 177.0\n",
      "4 3 202.0\n",
      "4 4 229.0\n",
      "4 5 258.0\n",
      "4 6 289.0\n",
      "4 7 322.0\n",
      "4 8 357.0\n",
      "4 9 394.0\n",
      "5 0 314.5\n",
      "5 1 346.5\n",
      "5 2 380.5\n",
      "5 3 416.5\n",
      "5 4 454.5\n",
      "5 5 494.5\n",
      "5 6 536.5\n",
      "5 7 580.5\n",
      "5 8 626.5\n",
      "5 9 674.5\n",
      "6 0 573.0\n",
      "6 1 616.0\n",
      "6 2 661.0\n",
      "6 3 708.0\n",
      "6 4 757.0\n",
      "6 5 808.0\n",
      "6 6 861.0\n",
      "6 7 916.0\n",
      "6 8 973.0\n",
      "6 9 1032.0\n",
      "7 0 908.5\n",
      "7 1 962.5\n",
      "7 2 1018.5\n",
      "7 3 1076.5\n",
      "7 4 1136.5\n",
      "7 5 1198.5\n",
      "7 6 1262.5\n",
      "7 7 1328.5\n",
      "7 8 1396.5\n",
      "7 9 1466.5\n",
      "8 0 1321.0\n",
      "8 1 1386.0\n",
      "8 2 1453.0\n",
      "8 3 1522.0\n",
      "8 4 1593.0\n",
      "8 5 1666.0\n",
      "8 6 1741.0\n",
      "8 7 1818.0\n",
      "8 8 1897.0\n",
      "8 9 1978.0\n",
      "9 0 1810.5\n",
      "9 1 1886.5\n",
      "9 2 1964.5\n",
      "9 3 2044.5\n",
      "9 4 2126.5\n",
      "9 5 2210.5\n",
      "9 6 2296.5\n",
      "9 7 2384.5\n",
      "9 8 2474.5\n",
      "9 9 2566.5\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.float32)\n",
    "Y = np.array([3,5,7,9,11,13,15,17,19,21], dtype=np.float32)\n",
    "\n",
    "W = 3\n",
    "B = 1\n",
    "print(Cost())\n",
    "\n",
    "W = 7\n",
    "B = 5\n",
    "print(Cost())\n",
    "\n",
    "for W in range(10):\n",
    "    for B in range(10):\n",
    "        print(W, B, Cost())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Gradient Descent Algorithm(경사하강법)\n",
    "- Cost Function의 기울기가 최저가 되도록..\n",
    "\n",
    "> Gradient -> partial derivative<br>\n",
    "> $\\frac{\\partial}{\\partial w}cost(w, b) = \\frac{1}{m}  \\sum_{i=1}^{m}(x_{i}(x_{i}w+(b-y_{i})))$\n",
    "<br>\n",
    "> $\\frac{\\partial}{\\partial b}cost(w, b) = \\frac{1}{m}  \\sum_{i=1}^{m}(x_{i}w - y_{i} + b)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.float32)\n",
    "labels = np.array([3,5,7,9,11,13,15,17,19,21], dtype=np.float32)\n",
    "\n",
    "W, B = np.random.normal(), np.random.normal()\n",
    "\n",
    "def Hypothesis(x):\n",
    "    return W*x + B\n",
    "\n",
    "def Cost():\n",
    "    return np.mean((Hypothesis(x_input) - labels)**2)\n",
    "\n",
    "def Gradient(x, y):\n",
    "    return np.mean(x*(x*W + (B-y))), np.mean((W*x-y+B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 W = -0.282, B = 0.597 Cost = 210.673\n",
      " 1000 W = 2.009, B = 0.941 Cost = 0.001\n",
      " 2000 W = 2.007, B = 0.952 Cost = 0.000\n",
      " 3000 W = 2.006, B = 0.961 Cost = 0.000\n",
      " 4000 W = 2.005, B = 0.968 Cost = 0.000\n",
      " 5000 W = 2.004, B = 0.974 Cost = 0.000\n",
      " 6000 W = 2.003, B = 0.979 Cost = 0.000\n",
      " 7000 W = 2.002, B = 0.983 Cost = 0.000\n",
      " 8000 W = 2.002, B = 0.986 Cost = 0.000\n",
      " 9000 W = 2.002, B = 0.989 Cost = 0.000\n",
      "10000 W = 2.001, B = 0.991 Cost = 0.000\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 217 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10000\n",
    "learning_rate = 0.001\n",
    "\n",
    "for cnt in range(0, epochs+1):\n",
    "    if cnt%(epochs//10) == 0:\n",
    "        print(f\"{cnt:5} W = {W:.3f}, B = {B:.3f} Cost = {Cost():.3f}\")\n",
    "    \n",
    "    grad_w, grad_b = Gradient(x_input, labels)\n",
    "    W-=learning_rate*grad_w\n",
    "    B-=learning_rate*grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p 57"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
